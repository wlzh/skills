#!/usr/bin/env python3
"""
YouTube to Blog Post Script - SEO Optimized Version
Convert YouTube videos to Hexo blog posts with enhanced SEO
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
import subprocess

try:
    import yt_dlp
except ImportError:
    print("Installing yt-dlp...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "yt-dlp"])
    import yt_dlp


# SEO optimization constants
MAX_DESCRIPTION_LENGTH = 160  # Google SEO recommended
MAX_KEYWORDS = 8  # Optimal number for SEO
MIN_KEYWORD_LENGTH = 2
MAX_KEYWORD_LENGTH = 20

# Stop words to filter from keywords
STOP_WORDS = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
    'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
    'è‡ªå·±', 'è¿™', 'https', 'http', 'www', 'com', 'cn', 'org', 'net', '://', '//',
    'åŠ å…¥', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å¯ä»¥', 'ä½¿ç”¨', 'é€šè¿‡', 'è¿›è¡Œ', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥',
}

# Common tech keyword mappings for better SEO
KEYWORD_SYNONYMS = {
    'ai': ['AI', 'äººå·¥æ™ºèƒ½', 'artificial intelligence'],
    'vps': ['VPS', 'è™šæ‹ŸæœåŠ¡å™¨', 'virtual private server'],
    'cdn': ['CDN', 'å†…å®¹åˆ†å‘ç½‘ç»œ', 'content delivery network'],
    'ssl': ['SSL', 'HTTPS', 'å®‰å…¨è¯ä¹¦'],
    'åŸŸå': ['domain', 'åŸŸåæ³¨å†Œ'],
    'æœåŠ¡å™¨': ['server', 'ä¸»æœº'],
    'å…è´¹': ['free', '0æˆæœ¬', 'å…è´¹èµ„æº'],
    'æ•™ç¨‹': ['tutorial', 'æŒ‡å—', 'guide', 'how to'],
}


def load_config(config_path=None, blog_dir=None):
    """Load configuration from config file or use defaults"""
    config = {
        "posts_dir": "source/_posts",
        "default_category": "æŠ€æœ¯",
        "default_tags": ["è§†é¢‘æ•™ç¨‹"],
        "author": "M.",
        "image_cdn": "https://img.869hr.uk"
    }

    if config_path and os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            user_config = json.load(f)
            config.update(user_config)
    elif blog_dir:
        config_path = os.path.join(blog_dir, "youtube-blog-config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                config.update(user_config)

    return config


def sanitize_text_for_yaml(text):
    """
    Sanitize text for YAML format
    Remove or escape special characters that break YAML parsing
    """
    if not text:
        return text

    # Remove YAML special characters
    text = re.sub(r'[*&:!|>]', '', text)  # Remove problematic chars
    text = re.sub(r'[ã€ã€‘ã€Šã€‹ã€Œã€ã€ã€ï¼ˆï¼‰()]', '', text)  # Remove Chinese brackets
    text = re.sub(r'[\[\]{}]', '', text)  # Remove brackets
    text = text.strip()

    return text


def clean_keywords(keywords):
    """Clean and optimize keywords for SEO"""
    cleaned = []

    for kw in keywords:
        # Remove special characters
        kw = sanitize_text_for_yaml(kw)

        # Skip if too short or too long
        if len(kw) < MIN_KEYWORD_LENGTH or len(kw) > MAX_KEYWORD_LENGTH:
            continue

        # Skip stop words
        if kw.lower() in STOP_WORDS:
            continue

        # Skip URLs
        if kw.startswith(('http', 'https', 'www', '//')):
            continue

        # Skip if starts with special chars
        if re.match(r'^[^\w]', kw):
            continue

        cleaned.append(kw)

    return cleaned


def get_video_info(url):
    """Fetch video information from YouTube"""
    print(f"Fetching video info from: {url}")

    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': False,
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            return {
                'id': info.get('id', ''),
                'title': sanitize_text_for_yaml(info.get('title', 'Untitled')),
                'description': info.get('description', ''),
                'uploader': info.get('uploader', ''),
                'upload_date': info.get('upload_date', ''),
                'duration': info.get('duration', 0),
                'thumbnail': info.get('thumbnail', ''),
                'tags': info.get('tags', []),
                'view_count': info.get('view_count', 0),
            }
        except Exception as e:
            print(f"Error fetching video info: {e}")
            return None


def extract_video_id(url):
    """Extract video ID from various YouTube URL formats"""
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)',
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def generate_english_filename(title, video_id=None):
    """Generate SEO-friendly English filename from video title"""
    # Enhanced keyword mapping for better SEO
    keyword_map = {
        # Common words
        'æ•™ç¨‹': 'tutorial',
        'æŒ‡å—': 'guide',
        'å·¥å…·': 'tools',
        'æŠ€æœ¯': 'tech',
        'å…è´¹': 'free',
        'åŸŸå': 'domain',
        'æœåŠ¡å™¨': 'server',
        'æ”¯ä»˜': 'payment',
        'å¡': 'card',
        'é“¶è¡Œå¡': 'bank-card',
        'è™šæ‹Ÿ': 'virtual',
        'æ³¨å†Œ': 'register',
        'ç”³è¯·': 'apply',
        'è®¾ç½®': 'setup',
        'é…ç½®': 'config',
        'å¼€å‘': 'dev',
        'å­¦ä¹ ': 'study',
        'å…¥é—¨': 'beginner',
        'è¿›é˜¶': 'advanced',
        'è¯¦è§£': 'detail',
        'ä½¿ç”¨': 'usage',
        'å®‰è£…': 'install',
        'éƒ¨ç½²': 'deploy',
        'æµ‹è¯•': 'test',
        'ä¼˜åŒ–': 'optimize',
        'åˆ†æ': 'analysis',
        'ä»‹ç»': 'intro',
        'ä»€ä¹ˆæ˜¯': 'what-is',
        'å¦‚ä½•': 'how-to',
        'ä¸ºä»€ä¹ˆ': 'why',
        'æœ€ä½³': 'best',
        'æ¨è': 'recommend',
        'å¯¹æ¯”': 'compare',
        'è¯„æµ‹': 'review',
        'çš„': '',
        'å’Œ': 'and',
        'ä¸': 'and',
        'æˆ–': 'or',
        'ï¼ˆ': '-',
        'ï¼‰': '',
        '(': '-',
        ')': '',
        'ï¼š': '-',
        ':': '-',
        'ï¼': '',
        '!': '',
        'ï¼Ÿ': '',
        '?': '',
        'ï¼Œ': '-',
        ',': '-',
        'ã€‚': '',
        '.': '',
        ' ': '-',
        '--': '-',
        'ã€': '',
        'ã€‘': '',
        'ã€Š': '',
        'ã€‹': '',
        # Tech terms
        'ç§‘å­¦ä¸Šç½‘': 'vpn',
        'vps': 'vps',
        'ai': 'ai',
        'äººå·¥æ™ºèƒ½': 'ai',
        'æœºå™¨å­¦ä¹ ': 'ml',
        'æ·±åº¦å­¦ä¹ ': 'deep-learning',
        'åŒºå—é“¾': 'blockchain',
        'äº‘è®¡ç®—': 'cloud',
        'å®¹å™¨åŒ–': 'docker',
        'å¾®æœåŠ¡': 'microservice',
        'å‰ç«¯': 'frontend',
        'åç«¯': 'backend',
        'å…¨æ ˆ': 'fullstack',
    }

    # Replace Chinese and special characters
    filename = title.lower()
    for cn, en in keyword_map.items():
        filename = filename.replace(cn, en)

    # Remove any remaining non-ASCII characters
    filename = re.sub(r'[^\x00-\x7f]', '', filename)
    filename = re.sub(r'[^\w\s-]', '', filename)
    filename = re.sub(r'[\s]+', '-', filename)
    filename = filename.strip('-')

    # If filename is too short or empty, use video ID
    if len(filename) < 5 or not filename:
        filename = f"youtube-{video_id}" if video_id else "video-post"

    # Limit length for SEO (shorter URLs are better)
    if len(filename) > 50:
        filename = filename[:50].rstrip('-')

    return filename


def generate_seo_keywords(title, description, tags):
    """
    Generate SEO-optimized keywords
    Focus on long-tail keywords and search intent
    """
    keywords = set()

    # 1. Extract from title (highest priority)
    title_words = re.split(r'[,ï¼Œã€\s]+', title)
    for word in title_words:
        word = word.strip()
        if len(word) >= 2:
            keywords.add(word)

    # 2. Add user-provided tags
    if isinstance(tags, list):
        keywords.update(tags)
    elif isinstance(tags, str):
        keywords.update([t.strip() for t in tags.split(',')])

    # 3. Extract high-value keywords from description
    if description:
        # Look for patterns like "å…³é”®è¯ï¼šxxx" or "å…³é”®è¯: xxx"
        keyword_pattern = r'å…³é”®è¯[ï¼š:]\s*([^\n]+)'
        keyword_matches = re.findall(keyword_pattern, description)
        for match in keyword_matches:
            kws = re.split(r'[,ï¼Œã€\s]+', match)
            keywords.update([kw.strip() for kw in kws if kw.strip()])

        # Extract meaningful phrases from description
        desc_sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', description)[:2]
        for sentence in desc_sentences:
            # Look for tech terms and product names
            tech_terms = re.findall(r'[A-Z]{2,}|[A-Za-z]{3,}', sentence)
            keywords.update(tech_terms)

    # Clean and optimize keywords
    keywords = clean_keywords(list(keywords))

    # Add related keywords for better SEO coverage
    final_keywords = []
    for kw in keywords[:MAX_KEYWORDS]:
        final_keywords.append(kw)

        # Add synonyms for important keywords
        if kw.lower() in KEYWORD_SYNONYMS:
            for synonym in KEYWORD_SYNONYMS[kw.lower()][:2]:
                if synonym not in final_keywords and len(final_keywords) < MAX_KEYWORDS:
                    final_keywords.append(synonym)

    return final_keywords[:MAX_KEYWORDS]


def generate_seo_description(title, description):
    """
    Generate SEO-optimized description
    - Max 160 characters (Google snippet length)
    - Include main keywords
    - Compelling call-to-action
    """
    # Clean up description
    if description:
        # Remove URLs, email addresses
        desc = re.sub(r'https?://\S+', '', description)
        desc = re.sub(r'\S+@\S+', '', desc)

        # Remove excessive whitespace
        desc = re.sub(r'\s+', ' ', desc).strip()

        # Get first meaningful sentence
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', desc)
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20 and len(sentence) <= MAX_DESCRIPTION_LENGTH:
                return sentence

    # Fallback to title-based description
    desc_template = f"æœ¬è§†é¢‘è¯¦ç»†ä»‹ç»{title}ï¼Œå¸®åŠ©ä½ å¿«é€Ÿäº†è§£å’ŒæŒæ¡ç›¸å…³çŸ¥è¯†ç‚¹ã€‚"
    if len(desc_template) > MAX_DESCRIPTION_LENGTH:
        desc_template = f"{title} - è¯¦ç»†æ•™ç¨‹ä¸æŒ‡å—"

    return desc_template[:MAX_DESCRIPTION_LENGTH]


def generate_post_content(video_info, config, category, tags):
    """Generate SEO-optimized blog post content"""

    video_id = video_info['id']
    title = video_info['title']
    description = video_info.get('description', '')
    thumbnail = video_info.get('thumbnail', '')

    # Generate SEO-optimized metadata
    keywords = generate_seo_keywords(title, description, tags)
    post_description = generate_seo_description(title, description)

    # Use current time as post date (not video upload time)
    date_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Generate tags list
    tag_list = tags if isinstance(tags, list) else [t.strip() for t in tags.split(',')]

    # Build cover image URL from YouTube thumbnail
    # Use high-quality thumbnail (maxresdefault)
    cover_image = f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"

    # Build front matter with SEO fields
    front_matter = f"""---
title: {title}
subtitle: {title}
date: {date_str}
updated: {date_str}
author: {config['author']}
description: {post_description}
categories:
  - {category}
{generate_tags_yaml(tag_list)}
keywords:
{generate_keywords_yaml(keywords)}
cover: {cover_image}
thumbnail: {cover_image}
toc: true
comments: true
copyright: true
---

"""

    # Generate article summary
    summary = generate_article_summary(video_info)

    # Generate video iframe with SEO attributes
    video_iframe = f"""## è§†é¢‘æ•™ç¨‹

<iframe width="560" height="315" src="https://www.youtube.com/embed/{video_id}" title="{title}" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

"""

    # Generate article content with SEO structure
    content = generate_article_content(video_info, video_id)

    # Generate reference links
    references = f"""

## å‚è€ƒé“¾æ¥

- [YouTubeè§†é¢‘åŸåœ°å€](https://www.youtube.com/watch?v={video_id})
- [ç›¸å…³æ¨è](https://869hr.uk)

---
"""

    return front_matter + summary + video_iframe + content + references


def generate_tags_yaml(tags):
    """Generate tags in YAML format"""
    if not tags:
        return "tags:"
    yaml = "tags:\n"
    for tag in tags:
        yaml += f"  - {tag}\n"
    return yaml


def generate_keywords_yaml(keywords):
    """Generate keywords in YAML format"""
    if not keywords:
        return ""
    yaml = ""
    for kw in keywords:
        # Escape special YAML characters
        kw_safe = sanitize_text_for_yaml(kw)
        yaml += f"  - {kw_safe}\n"
    return yaml


def generate_article_summary(video_info):
    """Generate compelling article summary"""
    title = video_info['title']
    description = video_info.get('description', '')
    duration = video_info.get('duration', 0)

    # Convert duration to minutes
    duration_min = duration // 60 if duration else 0

    if description:
        # Use first meaningful paragraph from description
        lines = description.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 20:
                summary = line[:300]
                return f"""<!-- æ–‡ç« æ‘˜è¦ -->
{{% blockquote %}}
{summary}...
{{% endblockquote %}}

"""
    return f"""<!-- æ–‡ç« æ‘˜è¦ -->
{{% blockquote %}}
æœ¬è§†é¢‘è¯¦ç»†ä»‹ç»{title}ï¼ˆæ—¶é•¿çº¦{duration_min}åˆ†é’Ÿï¼‰ï¼Œå¸®åŠ©ä½ å¿«é€Ÿäº†è§£å’ŒæŒæ¡ç›¸å…³çŸ¥è¯†ç‚¹ã€‚
{{% endblockquote %}}

"""


def generate_article_content(video_info, video_id):
    """Generate SEO-optimized article content"""
    title = video_info['title']
    uploader = video_info.get('uploader', '')
    description = video_info.get('description', '')
    duration = video_info.get('duration', 0)
    tags = video_info.get('tags', [])

    # Convert duration to minutes
    duration_min = duration // 60 if duration else 0

    content = f"""## è§†é¢‘ä»‹ç»

æœ¬è§†é¢‘ç”± {uploader} åˆ¶ä½œï¼Œæ—¶é•¿çº¦ {duration_min åˆ†é’Ÿã€‚

"""

    # Try to extract meaningful content from description
    # Remove common link lines and emojis
    desc_lines = []
    for line in description.split('\n'):
        line = line.strip()
        # Skip empty lines, links, and common social media lines
        if not line or line.startswith('http') or line.startswith('ğŸ’¬') or line.startswith('ğŸ”—'):
            continue
        if 'Telegram' in line and 't.me' in line:
            continue
        if 'Twitter' in line or 'x.com' in line:
            continue
        desc_lines.append(line)

    # Join meaningful lines
    meaningful_desc = '\n'.join(desc_lines[:30])  # Limit to first 30 meaningful lines

    # Extract timestamp chapters for better content structure
    timestamp_pattern = r'(\d{1,2}:\d{2})\s*[.\-]?\s*(.+)'
    timestamps = re.findall(timestamp_pattern, meaningful_desc)

    # Extract key features/benefits (usually marked with âœ… or similar)
    features = []
    for line in desc_lines:
        if 'âœ…' in line or 'æ ¸å¿ƒ' in line or 'äº®ç‚¹' in line:
            features.append(line.replace('âœ…', '').strip())

    if features:
        content += "## æ ¸å¿ƒäº®ç‚¹\n\n"
        for feature in features[:6]:
            # Clean up the feature text
            feature = re.sub(r'[ğŸš€ğŸ’¡âœ…ğŸ¯]', '', feature)
            feature = feature.replace('**', '').strip()
            if feature:
                content += f"**{feature}**\n\n"
        content += "\n"

    # Extract code examples if present
    code_blocks = re.findall(r'```[a-z]*\n(.*?)```', meaningful_desc, re.DOTALL)
    if code_blocks:
        content += "## é…ç½®ç¤ºä¾‹\n\n"
        for code in code_blocks[:2]:
            content += f"```\n{code.strip()}\n```\n\n"

    content += "## å‚è€ƒé“¾æ¥

- [YouTubeè§†é¢‘åŸåœ°å€](https://www.youtube.com/watch?v={})
- [ç›¸å…³æ¨è](https://869hr.uk)

---
""".format(video_id)

    return content


def humanize_article(content, video_title):
    """
    Apply humanizer to remove AI-generated writing patterns
    Simplified implementation based on humanizer-zh skill guidelines
    """
    lines = content.split('\n')
    humanized_lines = []
    skip_frontmatter = False
    frontmatter_end = False

    for i, line in enumerate(lines):
        # Skip front matter
        if line.strip() == '---':
            if not skip_frontmatter:
                skip_frontmatter = True
                humanized_lines.append(line)
                continue
            elif skip_frontmatter and not frontmatter_end:
                frontmatter_end = True
                humanized_lines.append(line)
                continue

        # Don't modify front matter
        if skip_frontmatter and not frontmatter_end:
            humanized_lines.append(line)
            continue

        # Skip video iframe
        if '<iframe' in line:
            humanized_lines.append(line)
            continue

        # Remove excessive emoji from headers
        if line.startswith('##'):
            # Remove multiple emojis from headings, keep at most one if relevant
            line = re.sub(r'[ğŸ¯ğŸ“¹ğŸ“ºğŸ’¡ğŸ“ğŸ“ğŸ“šğŸ”ğŸ”—]+', '', line)
            # Remove trailing colon
            line = re.sub(r':\s*$', '', line)
            humanized_lines.append(line)
            continue

        # Remove or replace AI patterns
        # Remove excessive emphasis
        line = re.sub(r'\*\*(.*?)\*\*', r'\1', line)  # Remove bold markdown
        line = re.sub(r'ğŸš€\*\*', '', line)  # Remove rocket emoji

        # Simplify overly promotional language
        line = re.sub(r'çœŸæ­£[çš„]', '', line)
        line = re.sub(r'ç»ˆæ[""]', '', line)
        line = re.sub(r'"ç™½å«–[""]', 'å…è´¹', line)
        line = re.sub(r'æ— é™æµé‡', '', line)
        line = re.sub(r'æ— é™ç”Ÿæˆ', '', line)
        line = re.sub(r'èŠ‚ç‚¹æ— é™', '', line)
        line = re.sub(r'4Kç§’å¼€', 'é€Ÿåº¦å¿«', line)

        # Remove AI vocabulary
        line = re.sub(r'æ­¤å¤–ï¼Œ', '', line)
        line = re.sub(r'æ·±å…¥æ¢è®¨', 'ä»‹ç»', line)
        line = re.sub(r'æ ¸å¿ƒçŸ¥è¯†', 'å†…å®¹', line)
        line = re.sub(r'å…³é”®ä¿¡æ¯', 'ä¿¡æ¯', line)
        line = re.sub(r'é‡è¦', '', line)
        line = re.sub(r'è‡³å…³é‡è¦çš„', '', line)
        line = re.sub(r'å¿…ä¸å¯å°‘çš„', '', line)

        # Simplify structure
        line = re.sub(r'æœ¬è§†é¢‘é€‚åˆä»¥ä¸‹è§‚ä¼—è§‚çœ‹ï¼š', 'é€‚åˆï¼š', line)
        line = re.sub(r'\*   ', '- ', line)  # Simplify bullet points

        # Remove repetitive title mentions
        if video_title in line and len(video_title) > 20:
            # If title appears verbatim in content, shorten it
            short_title = video_title[:30] + '...' if len(video_title) > 30 else video_title
            line = line.replace(video_title, 'è¿™ä¸ªæ•™ç¨‹')
            line = line.replace(short_title, 'è¿™ä¸ªæ•™ç¨‹')

        # Remove generic filler phrases
        line = re.sub(r'å»ºè®®åœ¨è§‚çœ‹è§†é¢‘æ—¶ï¼š', 'è§‚çœ‹æ—¶ï¼š', line)
        line = re.sub(r'æ— è®ºä½ æ˜¯â€¦â€¦éƒ½èƒ½ä»ä¸­è·å¾—æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚', '', line)

        humanized_lines.append(line)

    # Clean up excessive blank lines
    result = []
    prev_blank = False
    for line in humanized_lines:
        is_blank = line.strip() == ''
        if is_blank and prev_blank:
            continue
        result.append(line)
        prev_blank = is_blank

    return '\n'.join(result)


def save_post(content, filename, posts_dir, video_title, apply_humanizer=True):
    """Save blog post to file"""
    # Ensure directory exists
    posts_path = Path(posts_dir)
    posts_path.mkdir(parents=True, exist_ok=True)

    # Full file path
    file_path = posts_path / f"{filename}.md"

    # If file exists, add timestamp
    if file_path.exists():
        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
        file_path = posts_path / f"{filename}-{timestamp}.md"
        print(f"File exists, creating with timestamp: {file_path.name}")

    # Apply humanizer if enabled (default) - built-in implementation
    if apply_humanizer:
        print("ğŸ”„ Applying AI writing removal (natural language processing)...")
        content = humanize_article(content, video_title)
        print("âœ… Content humanized - SEO optimized and ready")

    # Write content
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"âœ… Blog post created: {file_path}")
    return file_path


def main():
    parser = argparse.ArgumentParser(
        description='Convert YouTube video to SEO-optimized Hexo blog post'
    )
    parser.add_argument('url', help='YouTube video URL')
    parser.add_argument('-b', '--blog-dir', help='Blog directory path')
    parser.add_argument('-c', '--category', default='æŠ€æœ¯', help='Post category')
    parser.add_argument('-t', '--tags', nargs='+', default=['è§†é¢‘æ•™ç¨‹'], help='Post tags')
    parser.add_argument('--config', help='Path to config JSON file')
    parser.add_argument('--posts-dir', help='Override posts directory')
    parser.add_argument('--dry-run', action='store_true', help='Generate content but don\'t save')
    parser.add_argument('--no-humanizer', action='store_true', help='Skip AI writing removal (humanizer)')

    args = parser.parse_args()

    # Load configuration
    config = load_config(args.config, args.blog_dir)

    # Determine posts directory
    if args.posts_dir:
        posts_dir = args.posts_dir
    elif args.blog_dir:
        posts_dir = os.path.join(args.blog_dir, config['posts_dir'])
    else:
        posts_dir = config['posts_dir']

    # Get video info
    video_info = get_video_info(args.url)
    if not video_info:
        print("âŒ Failed to fetch video information")
        return 1

    print(f"ğŸ“¹ Video: {video_info['title']}")
    print(f"ğŸ‘¤ Uploader: {video_info.get('uploader', 'N/A')}")

    # Generate filename
    video_id = extract_video_id(args.url) or video_info['id']
    filename = generate_english_filename(video_info['title'], video_id)
    print(f"ğŸ“ Filename: {filename}.md")

    # Generate post content
    content = generate_post_content(video_info, config, args.category, args.tags)

    if args.dry_run:
        print("\n" + "="*60)
        print("DRY RUN - Generated content:")
        print("="*60)
        print(content[:1500] + "..." if len(content) > 1500 else content)
        print(f"\nWould save to: {os.path.join(posts_dir, filename + '.md')}")
    else:
        # Save post (with humanizer by default)
        apply_humanizer = not args.no_humanizer
        file_path = save_post(content, filename, posts_dir, video_info['title'], apply_humanizer)
        print(f"\nğŸ‰ Post saved to: {file_path}")
        print(f"\nğŸ“‚ To deploy: cd {args.blog_dir or '.'} && hexo cl; hexo g; hexo d")

    return 0


if __name__ == '__main__':
    sys.exit(main())
